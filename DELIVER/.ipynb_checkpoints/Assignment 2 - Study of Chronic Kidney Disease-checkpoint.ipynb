{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DIGITAL EPIDEMIOLOGY\n",
    "\n",
    "\n",
    "# Assignment 2 - A study of the Chronic Kidney Disease\n",
    "\n",
    "\n",
    "# Giacomo Maretto, Gabriel Nespoli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 1: CORRELATION STUDY\n",
    "\n",
    "# 1.1\n",
    "\n",
    "Study the correlation between the ground truth data and the Google Trends data. Explore multiple features of the ground truth data (e.g., crude prevalence, age-adjusted prevalence, etc.), and multiple keywords or entity types for querying the Google Trends data. Check for stability (or lack there of) across different years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats.stats import pearsonr\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import Lasso,Ridge\n",
    "import random\n",
    "random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from a dataframe |keyword|correlation|, get a subset of this dataframe according to the threshold for the correlation.\n",
    "# It will return the keywords with correlation greater or equal than threshold.\n",
    "def get_most_correlated_terms_df(df, threshold=0.1):\n",
    "    # order the keywords by the correlation average value\n",
    "    df.sort_values(by=\"correlation\", ascending=False, inplace=True)\n",
    "    \n",
    "    # check if the threshold passed is a float and return a subset of the dataset, otherwise raise an exception\n",
    "    if isinstance(threshold, float):\n",
    "        return df.loc[df[\"correlation\"] >= threshold]\n",
    "    raise AttributeError(\"Exception in get_most_correlated_terms method. 'threshold' must be float.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all the dataframes of correlations and get the keywords with the best correlation between the \n",
    "# ground truth and google dataset, considering all the years in 'years'\n",
    "def get_compound_most_correlated_terms_df(file_type=\"AA\", \n",
    "                                          years = [2011,2012,2013,2014,2015,2016], threshold=0.1):\n",
    "    # create an empty dataset that will be filled and returned later\n",
    "    most_correlated_df = pd.DataFrame(columns=[\"keyword\",\"correlation\"])\n",
    "    \n",
    "    # sufix defines the right file to be read, according to the ground-truth data to be analyzed\n",
    "    sufix = \"\"\n",
    "    if file_type == 'AA':\n",
    "        sufix = \"_\" + file_type\n",
    "    \n",
    "    # iterate over the years populating the aforementioned empty dataframe with the keywords that\n",
    "    # have a high correlation with the GT data. The resulting dataframe will be a long table\n",
    "    # with words of each year that attend the condition 'correlation >= threshold'.\n",
    "    for year in years:\n",
    "        df = pd.read_csv('overall_correlation_'+ str(year) + sufix + '.csv', sep=';')\n",
    "        df = get_most_correlated_terms_df(df, threshold=threshold)\n",
    "        most_correlated_df = most_correlated_df.append(get_most_correlated_terms_df(df, threshold=threshold))\n",
    "    \n",
    "    # sort the dataframe by the correlation and drop the repeated keywords, preserving the highest correlated copy\n",
    "    df.sort_values(by=\"correlation\", ascending=False, inplace=True)\n",
    "    most_correlated_df.drop_duplicates(subset=\"keyword\", keep=\"first\", inplace=True)\n",
    "    most_correlated_df.reset_index(drop=True, inplace=True)\n",
    "    return most_correlated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the correlation between the Google Trends keywords and the ground-truth data over the years.\n",
    "# The y-axis shows the correlation value (between 0 and 1), and the x-axis presents the keywords that\n",
    "# appear with a high correlation (>=threshold) in all the years. Each line of the graph represents a year.\n",
    "def plot_keywords_correlation(file_type=\"AA\", years = [2011,2012,2013,2014,2015,2016], threshold=0.1):\n",
    "    \n",
    "    # creates an empty dataframe that will be store the values of the plot\n",
    "    corr_keywords_plot = pd.DataFrame(columns=[\"keyword\", \"correlation\"])\n",
    "    years = list(reversed(years))  # the most recent year appears first\n",
    "    \n",
    "    # mark the first loop of the iteration. It will define if the corr_keywords_plot will be initialized or,\n",
    "    # if exists, updated.\n",
    "    first_execution = True\n",
    "    \n",
    "    # create a dataframe with the correlation containing only the terms that repeated in all 'years'. That is,\n",
    "    # an inner join is done.\n",
    "    for year in years:\n",
    "        corr_df = get_compound_most_correlated_terms_df(file_type=file_type, years = [year], threshold=threshold)\n",
    "        if first_execution:\n",
    "            corr_keywords_plot = corr_df\n",
    "            first_execution = False\n",
    "        else:\n",
    "            # do an inner join based on the keyword. Only keywords that appear in all the years\n",
    "            # will be in the plot\n",
    "            corr_keywords_plot = corr_keywords_plot.merge(corr_df, on=[\"keyword\"], how=\"inner\")\n",
    "    \n",
    "    # rename the columns, assigning the year to them\n",
    "    corr_keywords_plot.columns = [\"keyword\"] + years\n",
    "    \n",
    "    # initialize the plot variables\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    # plot the curve for each year\n",
    "    for year in years:\n",
    "        corr_keywords_plot[year].plot(ax=ax)\n",
    "    \n",
    "    # reduce the font size of the legend and move it outside the boxplot\n",
    "    ax.legend(fontsize=\"small\", bbox_to_anchor=(1, 1))\n",
    "    \n",
    "    # define the title and the labels\n",
    "    if file_type == \"AA\":\n",
    "        file_type = \"age-adjusted prevalence\"\n",
    "    elif file_type == \"CP\":\n",
    "        file_type = \"crude prevalence\"\n",
    "    plt.title(\"Correlation comparison for the \" + file_type + \" ground-truth data using Google Trends keywords\")\n",
    "    plt.ylabel(\"Correlation\")\n",
    "    plt.xlabel(\"Query\")\n",
    "    \n",
    "    # The plot prints in the x-axis just the position of the keywords (ie. 1,2,3..), not the keywords themselves.\n",
    "    # A workaround is to map the position to the keyword that it represents\n",
    "    x_label_sequence = list(range(len(corr_keywords_plot[\"keyword\"].values)))\n",
    "    plt.xticks(x_label_sequence, corr_keywords_plot[\"keyword\"].values, rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_keywords_correlation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above graph shows the searched terms on Google Trends with highest correlation with the ground-truth data (age-adjusted) in repeated years. This means that only these eight terms had a correlation with the ground-truth higher than 0.1 from 2011 to 2016."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With few counter examples, it is shown that the more recent is the year, the better is the correlation. This can be explained through the growing in the Internet access and usage. This recent increase was specially observed in the older and poorer population. The idea is that the population is more informed and using more Internet to search for symptoms, treatment and prevention of diseases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Studying deeper the terms, it comes out that they are very significant terms in the universe of the Chronic Kidney Disease, and this affirmation is confirmed by studies, available in the links below:\n",
    "\n",
    "high blood pressure: https://www.niddk.nih.gov/health-information/kidney-disease/chronic-kidney-disease-ckd/high-blood-pressure\n",
    "\n",
    "dialysis: https://emedicine.medscape.com/article/238798-treatment\n",
    "\n",
    "cor pulmonale: https://www.ncbi.nlm.nih.gov/pubmed/22669490\n",
    "\n",
    "fatty liver kidney: http://care.diabetesjournals.org/content/39/10/1830\n",
    "\n",
    "gout: https://www.kidney.org/atoz/content/gout\n",
    "\n",
    "nephroptosis: http://medsideen.bitballoon.com/urinology15/nephroptosis-nephro395.html\n",
    "\n",
    "renal insuficiency: https://www.ncbi.nlm.nih.gov/pubmed/16344727"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_keywords_correlation(file_type=\"CP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It was found five more terms with significant correlation with the ground-truth data (crude prevalence): diabetes insipidus, renal failure, lupus, diabetic and myotonic muscular dystrophy.\n",
    "\n",
    "diabetes insipidus: https://www.niddk.nih.gov/health-information/kidney-disease/diabetes-insipidus\n",
    "\n",
    "renal failure: https://emedicine.medscape.com/article/238798-overview\n",
    "\n",
    "lupus: https://www.kidney.org/atoz/content/lupus\n",
    "\n",
    "diabetic: https://www.kidney.org/news/newsroom/factsheets/Diabetes-And-CKD\n",
    "\n",
    "myotonic muscular dystrophy: https://www.ncbi.nlm.nih.gov/pubmed/29029879"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen the same relation through the years with the ground-truth crude prevalence dataset, except for the keywords gout, nephroptosis, renal insufficiency and myotonic muscular dystrophy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2\n",
    "\n",
    "Study the correlation of your target ground truth feature with income/poverty features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the list of states of the ground truth to be used (it varies according to the year)\n",
    "df_gt = pd.read_csv('2012_AA.csv', sep=\";\",usecols=[\"LocationDesc\"])\n",
    "df_gt.dropna(subset=['LocationDesc'], inplace=True)\n",
    "df_gt.drop(df_gt.tail(2).index, inplace=True)\n",
    "states = df_gt[\"LocationDesc\"]\n",
    "\n",
    "# Calculate the correlation between the average household income of the states and the ground-truth\n",
    "# data over the years.\n",
    "def get_income_correlation_df(gt_type=\"AA\", years = [2011,2012,2013,2014,2015,2016]):\n",
    "    corr = []\n",
    "    output_df = pd.DataFrame(columns=[\"year\", \"correlation\"])\n",
    "    income_df = pd.read_csv(\"income_per_vector_2011-2016.csv\", sep=\";\")\n",
    "    \n",
    "    # gt_type defines the GT that will be used to calculate the correlation with the income\n",
    "    if gt_type == \"AA\":\n",
    "        gt_type = \"_AA\"\n",
    "    if gt_type == \"CP\":\n",
    "        gt_type = \"\"\n",
    "    \n",
    "    #iterate over the years calculating the correlation of the income with each GT dataset\n",
    "    for year in years:\n",
    "        gt_filename = str(year) + gt_type + '.csv'\n",
    "        gt_df = pd.read_csv(gt_filename, sep=\";\", usecols=[\"LocationDesc\", \"Data_Value\"])\n",
    "        gt_df = gt_df.loc[gt_df[\"LocationDesc\"].isin(states)]  # guarantees that only the 51 states will be used\n",
    "        corr.append(pearsonr(income_df[str(year)], gt_df[\"Data_Value\"])[0])\n",
    "    \n",
    "    # the output is the correlation per year: |year|correlation|\n",
    "    output_df[\"year\"] = years\n",
    "    output_df[\"correlation\"] = corr\n",
    "    output_df.set_index(\"year\", inplace=True)\n",
    "    \n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the correlation between the average household income of the states of USA and the ground-truth data\n",
    "# over the years. The y-axis shows the correlation value (between 0 and 1), and the x-axis presents the years.\n",
    "# Each line of the graph represents a ground-truth dataset [age-adjusted prevalence, crude prevalence].\n",
    "def plot_income_correlation(years = [2011,2012,2013,2014,2015,2016]):\n",
    "    income_df = get_income_correlation_df()\n",
    "    income_df[\"Crude Prevalence\"] = get_income_correlation_df(gt_type=\"CP\")\n",
    "    \n",
    "    # rename the columns of the dataframe\n",
    "    income_df.columns = [\"age-adjusted prevalence\", \"crude prevalence\"]\n",
    "    \n",
    "    # plot the curve for each year\n",
    "    income_df.plot()\n",
    "    \n",
    "    # set title and labels of the graph\n",
    "    plt.title(\"Correlation comparison of the ground truth data and the average household income\")\n",
    "    plt.ylabel(\"Correlation\")\n",
    "    plt.xlabel(\"Year\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'income_per_vector_2011-2016.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-6aaeec17285c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_income_correlation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-1475c78350a8>\u001b[0m in \u001b[0;36mplot_income_correlation\u001b[0;34m(years)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Each line of the graph represents a ground-truth dataset [age-adjusted prevalence, crude prevalence].\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot_income_correlation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myears\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m2011\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2012\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2013\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2014\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2015\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2016\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mincome_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_income_correlation_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mincome_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Crude Prevalence\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_income_correlation_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgt_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"CP\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-79a31eeaaf88>\u001b[0m in \u001b[0;36mget_income_correlation_df\u001b[0;34m(gt_type, years)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mcorr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0moutput_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"year\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"correlation\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mincome_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"income_per_vector_2011-2016.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\";\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# gt_type defines the GT that will be used to calculate the correlation with the income\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    653\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    762\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m    983\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1603\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1605\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__ (pandas/_libs/parsers.c:4209)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source (pandas/_libs/parsers.c:8873)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'income_per_vector_2011-2016.csv' does not exist"
     ]
    }
   ],
   "source": [
    "plot_income_correlation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph exibit a negative correlation between the average household income of the state in United State and the incidence of the Chronic Kidney Disease. Important factors to sustain this relation is that infectious diseases, poor sanitation, inadequate supply of safe water, environmental pollutants and high concentrations of disease-transmitting vectors  play an important role in the development of CKD in low-income sites. African-Americans, historically a poorer population in USA, can have more than three times more risk to suffer from CKD.\n",
    "\n",
    "It was also found that there is a circular effect between the poverty and CKD, which means that the patients tends to decrease their income over time.\n",
    "\n",
    "The aforementioned affirmations are confirmed in the following studies:\n",
    "\n",
    "https://academic.oup.com/ckj/article/8/1/3/434867\n",
    "https://www.sciencedirect.com/science/article/pii/S2468024917304795\n",
    "https://www.kidney.org/news/newsroom/nr/Low-Income-Linked-to-Higher-Levels-of-Kidney-Disease\n",
    "\n",
    "Moreover, it is noticed that the correlation is higher when the age standardization is applied over the dataset, correcting the discrepancy in the age profile of the population of each state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"output_complete.csv\",sep=\";\") # loading dataframe build in previous steps.\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, validation = train_test_split(df, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In the following steps we selected three linear regression models : a simple linear model, Ridge and Lasso.\n",
    "### For each model we computed the mean squared error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spatial Cross validation\n",
    "In order to make spatial cross validation we used a sort of Leave-one(state)-out cross validation technique, for each state infact, we splitted the dataframe in train and test set and fitted the models and get a mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#df_gt = pd.read_csv('data/BRFSS/2011_AA.csv', sep=\";\")\n",
    "#states = df_gt[\"LocationDesc\"][0:51]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "mseLM = []\n",
    "mseLasso = []\n",
    "mseRidge = []\n",
    "# LOO CrossValidation\n",
    "\n",
    "trainS=train.drop([\"Year\"],axis=1)\n",
    "\n",
    "\n",
    "for state in states:\n",
    "    trainK = trainS.loc[train[\"State\"] == state]\n",
    "    testK = trainS.drop(trainK.index)\n",
    "\n",
    "\n",
    "\n",
    "    #LM\n",
    "    lm = linear_model.LinearRegression()\n",
    "    model = lm.fit(trainK.iloc[:,1:-1],trainK.iloc[:,-1])\n",
    "    predictions = lm.predict(testK.iloc[:,1:-1])\n",
    "    mseLM.append(np.mean((predictions - testK.iloc[:,-1])**2))\n",
    "\n",
    "    #Lasso\n",
    "    trainKnum = trainK.apply(pd.to_numeric, errors='coerce')\n",
    "    testKnum = testK.apply(pd.to_numeric, errors='coerce')\n",
    "    trainKnum.fillna(0, inplace=True)\n",
    "    testKnum.fillna(0, inplace=True)\n",
    "    \n",
    "    lassoReg = Lasso()\n",
    "    lassoReg.fit(trainK.iloc[:,1:-1],trainK.iloc[:,-1])\n",
    "    predLasso = lassoReg.predict(testK.iloc[:,1:-1])\n",
    "    mseLasso.append(np.mean((predLasso - testK.iloc[:,-1])**2))\n",
    "\n",
    "    #Ridge\n",
    "    ridgeReg = Ridge()\n",
    "    ridgeReg.fit(trainK.iloc[:,1:-1],trainK.iloc[:,-1])\n",
    "    predRidge = ridgeReg.predict(testK.iloc[:,1:-1])\n",
    "    mseRidge.append(np.mean((predRidge - testK.iloc[:,-1])**2))\n",
    "\n",
    "lmod = np.mean(mseLM)\n",
    "las = np.mean(mseLasso)\n",
    "ridge = np.mean(mseRidge)\n",
    "\n",
    "\n",
    "print(\"Mean mse LM:\",np.mean(mseLM))\n",
    "print(\"Mean mse Lasso:\",np.mean(mseLasso))\n",
    "print(\"Mean mse Ridge:\",np.mean(mseRidge))\n",
    "print(\"Mean mse in spatial cv: \", np.mean([np.mean(mseLM),np.mean(mseLasso),np.mean(mseRidge)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal cross validation\n",
    "In order to make a temporal cross validation, we divided in folds the dataset , one for each year and divided in train test set according to this division, computed the models in each fold and get a mean of the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mseLM = []\n",
    "mseLasso = []\n",
    "mseRidge = []\n",
    "trainY=train.drop([\"State\"],axis=1)\n",
    "\n",
    "\n",
    "for year in [2011,2012,2013,2014,2015,2016]:\n",
    "    trainK = trainY.loc[train.Year == year]\n",
    "    testK = trainY.drop(trainK.index)\n",
    "    \n",
    "    \n",
    "    #lm\n",
    "    lm = linear_model.LinearRegression()\n",
    "    model = lm.fit(trainK.iloc[:,1:-1],trainK.iloc[:,-1])\n",
    "    predictions = lm.predict(testK.iloc[:,1:-1])\n",
    "    mseLM.append(np.mean((predictions - testK.iloc[:,-1])**2))\n",
    "\n",
    "\n",
    "    \n",
    "    #Lasso\n",
    "    trainKnum = trainK.apply(pd.to_numeric, errors='coerce')\n",
    "    testKnum = testK.apply(pd.to_numeric, errors='coerce')\n",
    "    trainKnum.fillna(0, inplace=True)\n",
    "    testKnum.fillna(0, inplace=True)\n",
    "    \n",
    "    lassoReg = Lasso()\n",
    "    lassoReg.fit(trainK.iloc[:,1:-1],trainK.iloc[:,-1])\n",
    "    predLasso = lassoReg.predict(testK.iloc[:,1:-1])\n",
    "    mseLasso.append(np.mean((predLasso - testK.iloc[:,-1])**2))\n",
    "\n",
    "    #Ridge\n",
    "    ridgeReg = Ridge()\n",
    "    ridgeReg.fit(trainK.iloc[:,1:-1],trainK.iloc[:,-1])\n",
    "    predRidge = ridgeReg.predict(testK.iloc[:,1:-1])\n",
    "    mseRidge.append(np.mean((predRidge - testK.iloc[:,-1])**2))\n",
    "\n",
    "lm2 = np.mean(mseLM)\n",
    "las2 = np.mean(mseLasso)\n",
    "ridge2 = np.mean(mseRidge)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "print(\"Mean mse LM:\",np.mean(mseLM))\n",
    "print(\"Mean mse Lasso:\",np.mean(mseLasso))\n",
    "print(\"Mean mse Ridge:\",np.mean(mseRidge))\n",
    "print(\"Mean mse in temporal cv: \", np.mean([np.mean(mseLM),np.mean(mseLasso),np.mean(mseRidge)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Â Mean of each model error in the two cross validation types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"lm:\", np.mean([lmod,lm2]))\n",
    "print(\"lasso:\", np.mean([las,las2]))\n",
    "print(\"ridge:\", np.mean([ridge,ridge2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso was the best performing model so we selected it to train on all the train set and test it on the validation set (not used till now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lasso best model\n",
    "from sklearn.linear_model import Lasso\n",
    "lassoReg = Lasso(0.2)\n",
    "lassoReg.fit(train.iloc[:,1:-1],train.iloc[:,-1])\n",
    "pred2 = lassoReg.predict(validation.iloc[:,1:-1])\n",
    "# calculating mse\n",
    "mse = np.mean((pred2 - validation.iloc[:,-1])**2)\n",
    "print(\"mse: \",mse)\n",
    "print(\"R squared: \",lassoReg.score(validation.iloc[:,1:-1],validation.iloc[:,-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We let the Lasso linear model select the variables for us, giving them a null coefficients so making their predicting effect absent in the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = train.iloc[:,1:-1].columns\n",
    "coef = pd.Series(lassoReg.coef_,predictors).sort_values()\n",
    "plt.figure(figsize=(20,10))\n",
    "coef.plot(kind='bar', title='Modal Coefficients',grid=True)\n",
    "plt.show()\n",
    "print(lassoReg.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Added to our model census features at the US state level (income data)\n",
    "## Adding the income variable we notice from the mse values that the predicting performance of our lasso model got better.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "income= pd.read_csv(\"DELIVER/income.csv\", sep=\";\")\n",
    "_2011i = income.iloc[:,1]\n",
    "_2012i = income.iloc[:,2]\n",
    "_2013i = income.iloc[:,3]\n",
    "_2014i = income.iloc[:,4]\n",
    "_2015i = income.iloc[:,5]\n",
    "_2016i = income.iloc[:,6]\n",
    "income=pd.concat([_2011i,_2012i,_2013i,_2014i,_2015i,_2016i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"income\"] = income.iloc[train.index].values\n",
    "validation[\"income\"] = income.iloc[validation.index].values\n",
    "#pd.concat([train.income,validation.income],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "train = train.apply(pd.to_numeric, errors='coerce')\n",
    "validation = validation.apply(pd.to_numeric, errors='coerce')\n",
    "train.fillna(0, inplace=True)\n",
    "validation.fillna(0, inplace=True)\n",
    "lassoReg = Lasso(alpha=0.2)\n",
    "lassoReg.fit(train.drop([\"Data_Value\"],axis=1),train.Data_Value)\n",
    "pred2 = lassoReg.predict(validation.drop([\"Data_Value\"],axis=1))\n",
    "# calculating mse\n",
    "\n",
    "mse = np.mean((pred2 - validation.Data_Value)**2)\n",
    "print(\"mse :\",mse)\n",
    "print(\"R squared: \",lassoReg.score(validation.drop([\"Data_Value\"],axis=1),validation.Data_Value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lm = linear_model.LinearRegression()\n",
    "model = lm.fit(X_train,y_train)\n",
    "predictions = lm.predict(X_test)\n",
    "(np.mean((predictions - y_test)**2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As a result we get better results (mse reduction from 0.15 to 0.13) adding the \"income\" variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We also added a state Health Insurance coverage indicator, for all the years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "health = pd.read_csv(\"DELIVER/health.csv\",sep=(\";\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_2011 = health.iloc[:,1]\n",
    "_2012 = health.iloc[:,2]\n",
    "_2013 = health.iloc[:,3]\n",
    "_2014 = health.iloc[:,4]\n",
    "_2015 = health.iloc[:,5]\n",
    "_2016 = health.iloc[:,6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "health=pd.concat([_2011,_2012,_2013,_2014,_2015,_2016])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"health\"] = health.iloc[train.index].values\n",
    "validation[\"health\"] = health.iloc[validation.index].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lassoReg1 = Lasso(alpha=0.2)\n",
    "lassoReg1.fit(train.drop([\"Data_Value\"],axis=1),train.Data_Value)\n",
    "pred3 = lassoReg1.predict(validation.drop([\"Data_Value\"],axis=1))\n",
    "# calculating mse\n",
    "\n",
    "mse = np.mean((pred3 - validation.Data_Value)**2)\n",
    "print(\"mse :\",mse)\n",
    "print(\"R squared: \",lassoReg1.score(validation.drop([\"Data_Value\"],axis=1),validation.Data_Value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strangely we got exact same results of the previous model, so we can see that the model excluded the \"health\" variable giving a null coefficient, estimating not usefull for an improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Coefficient of health variable:\",lassoReg1.coef_[-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
