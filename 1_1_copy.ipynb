{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pytrends.request import TrendReq\n",
    "pytrends = TrendReq(hl='en-US', tz=360)\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats.stats import pearsonr\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "df_gt = pd.read_csv('data/BRFSS/2012_AA.csv', sep=\";\",usecols=[\"LocationDesc\"])\n",
    "df_gt.dropna(subset=['LocationDesc'], inplace=True)\n",
    "df_gt.drop(df_gt.tail(2).index, inplace=True)\n",
    "states = df_gt[\"LocationDesc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_google_trends_df(kword, states, years=[2011,2012,2013,2014,2015,2016], output=True):\n",
    "    if output:\n",
    "        print(\"calculating indexes for\", kword)\n",
    "    yearly_google_trends_df = pd.DataFrame(index=states, columns=years, dtype=np.float)\n",
    "    for year in years:\n",
    "        values = []\n",
    "        try:\n",
    "            pytrends.build_payload([kword], cat=0, timeframe=str(year)+'-01-01'+ ' ' + str(year) + '-12-31', geo='US', gprop='')\n",
    "            trends_per_region_df = pytrends.interest_by_region()\n",
    "            for state in states:\n",
    "                if state not in trends_per_region_df.index:\n",
    "                    values.append(0)\n",
    "                else: \n",
    "                    values.append(int(trends_per_region_df.loc[state].values))\n",
    "            yearly_google_trends_df[year] = values\n",
    "        except: \n",
    "            raise Exception('Not relevant keyword in GoogleTrends database')\n",
    "    return yearly_google_trends_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def overall_correlation_df(gt_filename, google_trends_keywords, years):\n",
    "    df_gt = pd.read_csv(gt_filename, sep=\";\",usecols=[\"LocationDesc\",\"Data_Value\"])\n",
    "    df_gt.dropna(subset=['LocationDesc'], inplace=True)\n",
    "    df_gt.drop(df_gt.tail(2).index, inplace=True)\n",
    "    states = df_gt[\"LocationDesc\"]\n",
    "    bad_keywords = []\n",
    "    year_average_corr = []\n",
    "    for kw in google_trends_keywords:\n",
    "        try:\n",
    "            df = build_google_trends_df(kw, states=states, years=years, output=True )\n",
    "            df[\"ground_truth_value\"] = df_gt[\"Data_Value\"].values\n",
    "            corr_per_year= []\n",
    "            for year in years:\n",
    "                corr = pearsonr(df[year], df[\"ground_truth_value\"])[0]\n",
    "                if np.isnan(corr):\n",
    "                    raise Exception('Pearson correlation resulted in NaN')\n",
    "                corr_per_year.append(corr) # pos0: corr; pos1: p_value\n",
    "            year_average_corr.append(np.mean(corr_per_year))\n",
    "        except:\n",
    "            bad_keywords.append(kw)\n",
    "    good_keywords = [kw for kw in google_trends_keywords if kw not in bad_keywords] # remove the bad keywords, ie. with no results in the period\n",
    "    corr_df = pd.DataFrame(index=good_keywords,data=year_average_corr, columns=[\"correlation\"])\n",
    "    return corr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wls = pd.read_csv(\"data/kidney_disease_related_terms.csv\",sep=\".\", names=[\"related_terms\"])\n",
    "kw_list = wls[\"related_terms\"].str.strip()\n",
    "years = [2011,2012,2013,2014,2015,2016]\n",
    "\n",
    "folder = 'data/BRFSS/'\n",
    "#files_sufix = ['2011_AA','2012_AA','2013_AA','2014_AA','2015_AA','2016_AA','2011','2012','2013','2014','2015','2016']\n",
    "files_sufix = ['2011']\n",
    "overall_corr_dict = {}\n",
    "for sufix in files_sufix:\n",
    "    df = overall_correlation_df(gt_filename=folder+sufix+'.csv',\n",
    "                                google_trends_keywords=kw_list,\n",
    "                                years = years)\n",
    "    df.to_csv('data/correlation/overall_correlation_' + sufix + '.csv',sep=\";\", index=True, header=True, index_label='keyword')\n",
    "    overall_corr_dict[sufix] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from a datafram |keyword|correlation|, get a subset of this dataframe according to the threshold, which can be float or int.\n",
    "# If it is float, it will return the keywords with correlation greater or equal then threshold. If it, it will return the first\n",
    "# threshold-terms. If not a float, nor a int is passed, raise an AttributeError.\n",
    "def get_most_correlated_terms_df(df, threshold=0.1):\n",
    "    # order the keywords by the correlation average value\n",
    "    df.sort_values(by=\"correlation\", ascending=False, inplace=True)\n",
    "    if isinstance(threshold, float):\n",
    "        return df.loc[df[\"correlation\"] >= threshold]\n",
    "    elif isinstance(threshold, int):\n",
    "        return df.head(threshold)\n",
    "    raise AttributeError(\"Exception in get_most_correlated_terms method. 'threshold' must be float or int.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from a datafram |keyword|correlation|, get a subset of this dataframe according to the threshold, which can be float or int.\n",
    "# If it is float, it will return the keywords with correlation greater or equal then threshold. If it, it will return the first\n",
    "# threshold-terms. If not a float, nor a int is passed, raise an AttributeError.\n",
    "def get_compound_most_correlated_terms_df(location='data/correlation/', file_type=\"AA\", \n",
    "                                          years = [2011,2012,2013,2014,2015,2016], threshold=0.1):\n",
    "    most_correlated_df = pd.DataFrame(columns=[\"keyword\",\"correlation\"])\n",
    "    \n",
    "    sufix = \"\"\n",
    "    if file_type == 'AA':\n",
    "        sufix = \"_\" + file_type\n",
    "    for year in years:\n",
    "        df = pd.read_csv(location + 'overall_correlation_'+ str(year) + sufix + '.csv', sep=';')\n",
    "        df = get_most_correlated_terms_df(df, threshold=threshold)\n",
    "        most_correlated_df = most_correlated_df.append(get_most_correlated_terms_df(df, threshold=threshold))\n",
    "    df.sort_values(by=\"correlation\", ascending=False, inplace=True)\n",
    "    most_correlated_df.drop_duplicates(subset=\"keyword\", keep=\"first\", inplace=True)\n",
    "    return most_correlated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_google_trends_most_correlated_terms_df_old(gt_filename, threshold, years = [2011,2012,2013,2014,2015,2016]):\n",
    "    df = pd.read_csv(gt_filename, sep=';')\n",
    "    most_correlated_keywords = (get_most_correlated_terms_df(df, threshold))[\"keyword\"].values\n",
    "    \n",
    "    print(\"With the threshold applied it was found \" + str(len(most_correlated_keywords)) + \" keywords.\")\n",
    "    \n",
    "    output_df = pd.DataFrame(columns=[\"State\", \"Year\"] + list(most_correlated_keywords))\n",
    "    first_iteration = True\n",
    "    for keyword in most_correlated_keywords:\n",
    "        df = build_google_trends_df(keyword, states, years=[2011,2012,2013,2014,2015,2016], output=True)\n",
    "        series = df.stack()  # convert columns into rows, it returns a series with just one column\n",
    "        series.to_csv(path=\"data/temp/temp.csv\", sep=\"\\t\", header=True)\n",
    "        df = pd.read_csv(\"data/temp/temp.csv\", sep=\"\\t\")\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "        df.columns = [\"State\", \"Year\", keyword]\n",
    "        df.sort_values(['Year','State'], inplace=True)\n",
    "\n",
    "        # in the first iteration it is necessary to populate the year and state columns\n",
    "        if first_iteration:\n",
    "            output_df = df\n",
    "            first_iteration = False\n",
    "        else:\n",
    "            output_df[keyword] = df[keyword]\n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_google_trends_most_correlated_terms_df(file_type=\"AA\", threshold=0.1, years = [2011,2012,2013,2014,2015,2016]):\n",
    "    # get the most correlated terms through all the years compering with all the ground truth data availabe (for each type,\n",
    "    # for example, AA or CP)\n",
    "    most_correlated_keywords = (get_compound_most_correlated_terms_df(location='data/correlation/', \n",
    "                                                                      file_type=\"AA\",\n",
    "                                                                      years = [2011,2012,2013,2014,2015,2016], \n",
    "                                                                      threshold=0.1))[\"keyword\"].values\n",
    "    \n",
    "    print(\"With the threshold applied it was found \" + str(len(most_correlated_keywords)) + \" keywords.\")\n",
    "    \n",
    "    output_df = pd.DataFrame(columns=[\"State\", \"Year\"] + list(most_correlated_keywords))\n",
    "    first_iteration = True\n",
    "    for keyword in most_correlated_keywords:\n",
    "        df = build_google_trends_df(keyword, states, years=[2011,2012,2013,2014,2015,2016], output=True)\n",
    "        series = df.stack()  # convert columns into rows, it returns a series with just one column\n",
    "        series.to_csv(path=\"data/temp/temp.csv\", sep=\"\\t\", header=True)\n",
    "        df = pd.read_csv(\"data/temp/temp.csv\", sep=\"\\t\")\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "        df.columns = [\"State\", \"Year\", keyword]\n",
    "        df.sort_values(['Year','State'], inplace=True)\n",
    "\n",
    "        # in the first iteration it is necessary to populate the year and state columns\n",
    "        if first_iteration:\n",
    "            output_df = df\n",
    "            first_iteration = False\n",
    "        else:\n",
    "            output_df[keyword] = df[keyword]\n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_regression_df(folder = 'data/BRFSS/', gt_type ='AA', threshold=0.1, years = [2011,2012,2013,2014,2015,2016]):\n",
    "    # age adjusted or crude prevalence\n",
    "    if gt_type = \"AA\":\n",
    "        gt_type = \"_AA\"\n",
    "    df_gt_output = pd.DataFrame(columns=[\"State\", \"Year\", \"Data_Value\"])\n",
    "    for year in years:\n",
    "        gt_filename = folder + str(year) + gt_type + '.csv'\n",
    "        df_gt = pd.read_csv(gt_filename, sep=\";\", usecols=[\"LocationDesc\", \"Data_Value\"])\n",
    "        df_gt[\"Year\"] = year  # repeat the year in the entire column \n",
    "        df_gt.dropna(subset=['Data_Value'], inplace=True)  # some values in the ground_truth are loaded wrongly\n",
    "        df_gt = df_gt[[\"LocationDesc\",\"Year\", \"Data_Value\"]]\n",
    "        df_gt.columns = [\"State\", \"Year\", \"Data_Value\"]  # rename the columns\n",
    "        df_gt_output = df_gt_output.append(df_gt)\n",
    "\n",
    "    # do a left join with the already existing google trends dataframe\n",
    "    final_output = pd.read_csv(\"data/output/google_trends_df.csv\", sep=\";\")\n",
    "    final_output = final_output.merge(df_gt_output, on=[\"State\", \"Year\"], how=\"inner\")\n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With the threshold applied it was found 108 keywords.\n",
      "calculating indexes for nephroptosis\n",
      "calculating indexes for necrobiosis lipoidica\n",
      "calculating indexes for renal insufficiency\n",
      "calculating indexes for high blood pressure\n",
      "calculating indexes for iron overload\n",
      "calculating indexes for pyelitis\n",
      "calculating indexes for pulmonary emphysema\n",
      "calculating indexes for gout\n",
      "calculating indexes for leptospirosis\n",
      "calculating indexes for kalemia\n",
      "calculating indexes for haematuria\n",
      "calculating indexes for mucoviscidosis\n",
      "calculating indexes for renal failure\n",
      "calculating indexes for fatty liver\n",
      "calculating indexes for rachischisis\n",
      "calculating indexes for metabolic disorder\n",
      "calculating indexes for coronary thrombosis\n",
      "calculating indexes for Christmas Disease\n",
      "calculating indexes for kidney\n",
      "calculating indexes for azotemia\n",
      "calculating indexes for lupus\n",
      "calculating indexes for cor pulmonale\n",
      "calculating indexes for dialysis\n",
      "calculating indexes for capsule\n",
      "calculating indexes for cirrhosis\n",
      "calculating indexes for diabetic\n",
      "calculating indexes for nephrosclerosis\n",
      "calculating indexes for polycystic kidney disease\n",
      "calculating indexes for sugar diabetes\n",
      "calculating indexes for anuria\n",
      "calculating indexes for kidney failure\n",
      "calculating indexes for nephrectomy\n",
      "calculating indexes for amyotrophic lateral sclerosis\n",
      "calculating indexes for cardiovascular disease\n",
      "calculating indexes for liver\n",
      "calculating indexes for nephrite\n",
      "calculating indexes for emia\n",
      "calculating indexes for albuminuria\n",
      "calculating indexes for nephrolith\n",
      "calculating indexes for aplastic anemia\n",
      "calculating indexes for coeliac disease\n",
      "calculating indexes for lymphoblastic leukemia\n",
      "calculating indexes for coronary artery disease\n",
      "calculating indexes for glaucoma\n",
      "calculating indexes for cardiovascular\n",
      "calculating indexes for aortic stenosis\n",
      "calculating indexes for renal\n",
      "calculating indexes for diabetes insipidus\n",
      "calculating indexes for uremia\n",
      "calculating indexes for nephritis\n",
      "calculating indexes for hypertrophic cardiomyopathy\n",
      "calculating indexes for hyponatremia\n",
      "calculating indexes for arteriosclerosis\n",
      "calculating indexes for respiratory disease\n",
      "calculating indexes for urine\n",
      "calculating indexes for myotonic muscular dystrophy\n",
      "calculating indexes for ray\n",
      "calculating indexes for chronic lymphocytic leukemia\n",
      "calculating indexes for toxemia of pregnancy\n",
      "calculating indexes for polydipsia\n",
      "calculating indexes for pancreatitis\n",
      "calculating indexes for degenerative\n",
      "calculating indexes for congestive heart failure\n",
      "calculating indexes for gulf war syndrome\n",
      "calculating indexes for uropathy\n",
      "calculating indexes for anemia\n",
      "calculating indexes for hemodialysis\n",
      "calculating indexes for pyelonephritis\n",
      "calculating indexes for renal corpuscle\n",
      "calculating indexes for genetic disorder\n",
      "calculating indexes for liver cancer\n",
      "calculating indexes for inflammatory bowel disease\n",
      "calculating indexes for acute myeloid leukemia\n",
      "calculating indexes for nephrotic syndrome\n",
      "calculating indexes for sickle cell anemia\n",
      "calculating indexes for oliguria\n",
      "calculating indexes for hypertension\n",
      "calculating indexes for stoning\n",
      "calculating indexes for rheumatoid arthritis\n",
      "calculating indexes for histiocytosis\n",
      "calculating indexes for hydronephrosis\n",
      "calculating indexes for autoimmune disease\n",
      "calculating indexes for Pkd\n",
      "calculating indexes for systemic lupus erythematosus\n",
      "calculating indexes for sleep apnea\n",
      "calculating indexes for nephropathy\n",
      "calculating indexes for prostate cancer\n",
      "calculating indexes for pneumonia\n",
      "calculating indexes for conversion disorder\n",
      "calculating indexes for glomerulonephritis\n",
      "calculating indexes for colon cancer\n",
      "calculating indexes for hemolytic anemia\n",
      "calculating indexes for lichen planus\n",
      "calculating indexes for lupus erythematosus\n",
      "calculating indexes for viscus\n",
      "calculating indexes for azathioprine\n",
      "calculating indexes for cancer\n",
      "calculating indexes for breast cancer\n",
      "calculating indexes for hyperkalemia\n",
      "calculating indexes for sepsis\n",
      "calculating indexes for pemphigus\n",
      "calculating indexes for erythropoietin\n",
      "calculating indexes for diabetes mellitus\n",
      "calculating indexes for Kawasaki Disease\n",
      "calculating indexes for chronic renal failure\n",
      "calculating indexes for rheumatic fever\n",
      "calculating indexes for ballottement\n",
      "calculating indexes for rheumatism\n"
     ]
    }
   ],
   "source": [
    "google_trends = get_google_trends_most_correlated_terms_df(file_type=\"AA\", \n",
    "                                                           threshold=0.1, \n",
    "                                                           years = [2011,2012,2013,2014,2015,2016])\n",
    "google_trends.to_csv(\"data/output/google_trends_df.csv\", index=False, sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_complete = build_regression_df()\n",
    "output_complete.to_csv(\"data/output/output_complete.csv\", index=False, sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(13,7))\n",
    "plt.plot(corr_df, label =\"Age Adjusted prevalence\")\n",
    "\n",
    "plt.legend(fontsize=\"small\")\n",
    "plt.xlabel(\"query term\")\n",
    "plt.ylabel(\"Cor value \")\n",
    "plt.title(\"Correlation\")\n",
    "plt.grid()\n",
    "#plt.xticks(queries)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>renal</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>geoName</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Alabama</th>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Alaska</th>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Arizona</th>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Arkansas</th>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>California</th>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Colorado</th>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Connecticut</th>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Delaware</th>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>District of Columbia</th>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Florida</th>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Georgia</th>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hawaii</th>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Idaho</th>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Illinois</th>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Indiana</th>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Iowa</th>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kansas</th>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kentucky</th>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Louisiana</th>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Maine</th>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Maryland</th>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Massachusetts</th>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Michigan</th>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Minnesota</th>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mississippi</th>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Missouri</th>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Montana</th>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nebraska</th>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nevada</th>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>New Hampshire</th>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>New Jersey</th>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>New Mexico</th>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>New York</th>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>North Carolina</th>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>North Dakota</th>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ohio</th>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Oklahoma</th>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Oregon</th>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pennsylvania</th>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rhode Island</th>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>South Carolina</th>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>South Dakota</th>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tennessee</th>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Texas</th>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Utah</th>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vermont</th>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Virginia</th>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Washington</th>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>West Virginia</th>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wisconsin</th>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wyoming</th>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      renal\n",
       "geoName                    \n",
       "Alabama                  73\n",
       "Alaska                   54\n",
       "Arizona                  69\n",
       "Arkansas                 71\n",
       "California               47\n",
       "Colorado                 53\n",
       "Connecticut              65\n",
       "Delaware                 63\n",
       "District of Columbia     52\n",
       "Florida                  64\n",
       "Georgia                  57\n",
       "Hawaii                   59\n",
       "Idaho                    57\n",
       "Illinois                 57\n",
       "Indiana                  72\n",
       "Iowa                     68\n",
       "Kansas                   60\n",
       "Kentucky                 75\n",
       "Louisiana                79\n",
       "Maine                    53\n",
       "Maryland                 72\n",
       "Massachusetts            67\n",
       "Michigan                 71\n",
       "Minnesota                64\n",
       "Mississippi              77\n",
       "Missouri                 70\n",
       "Montana                  46\n",
       "Nebraska                 77\n",
       "Nevada                   54\n",
       "New Hampshire            63\n",
       "New Jersey               58\n",
       "New Mexico               82\n",
       "New York                 57\n",
       "North Carolina           67\n",
       "North Dakota             62\n",
       "Ohio                     73\n",
       "Oklahoma                 82\n",
       "Oregon                   58\n",
       "Pennsylvania             74\n",
       "Rhode Island             59\n",
       "South Carolina           71\n",
       "South Dakota             84\n",
       "Tennessee                98\n",
       "Texas                    59\n",
       "Utah                     52\n",
       "Vermont                  67\n",
       "Virginia                 56\n",
       "Washington               47\n",
       "West Virginia           100\n",
       "Wisconsin                68\n",
       "Wyoming                  62"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "year = 2011\n",
    "kword = \"renal\"\n",
    "pytrends.build_payload([kword], cat=0, timeframe=str(year)+'-01-01'+ ' ' + str(year) + '-12-31', geo='US', gprop='')\n",
    "trends_per_region_df = pytrends.interest_by_region()\n",
    "trends_per_region_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
